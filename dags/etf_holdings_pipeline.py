"""
## ETF Holdings Pipeline

This DAG calculates daily ETF holdings values:
1. Reads ETF market data (prices) from etf_market_data table
2. Reads exchange rates from exchange_rates table
3. Reads ETF trades (shares) from etf_trades table
4. Calculates USD prices for all ETFs
5. Calculates holdings values (price √ó shares)
6. Writes results to etf_holdings table
7. Sends success notification webhook

The pipeline processes data for a specific business date and calculates
the total value of ETF holdings in USD.
"""

from datetime import datetime, timedelta
from typing import List, Dict, Any
import logging
import json

from airflow import DAG
from airflow.decorators import dag, task
from airflow.providers.postgres.hooks.postgres import PostgresHook
import requests


def send_success_webhook(context, holdings_processed, total_holdings_value, business_date):
    """
    Send success webhook notification to the monitoring endpoint
    
    Args:
        context: Airflow context containing DAG run and task instance details
        holdings_processed: Number of holdings written to database
        total_holdings_value: Total value of all holdings
        business_date: The business date for the holdings
    """
    print("üîî Sending DAG success webhook notification")
    logging.info("Sending DAG success webhook notification")
    
    try:
        # Extract context information
        dag_run = context.get('dag_run')
        dag = context.get('dag')
        execution_date = context.get('execution_date') or context.get('logical_date')
        
        # Prepare webhook payload
        webhook_payload = {
            "event_type": "dag_success",
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "dag_id": dag.dag_id,
            "task_id": "write_etf_holdings",  # Task that triggered the webhook
            "run_id": dag_run.run_id,
            "organization_id": "cmfhfglh40k9h01rbszv8zhxx",
            "deployment_id": "cmfhfmdq70kb601rb5wu0d2nd",
            "dag_details": {
                "dag_id": dag.dag_id,
                "execution_date": execution_date.isoformat(),
                "state": "success",
                "log_url": f"https://cmfhfglh40k9h01rbszv8zhxx.astronomer.run/dwu0d2nd/dags/{dag.dag_id}/grid",
                "holdings_processed": holdings_processed,
                "total_holdings_value": float(total_holdings_value),
                "business_date": business_date
            },
            "environment": "production"
        }
        
        print(f"üì¶ DAG Success Webhook payload: {json.dumps(webhook_payload, indent=2)}")
        
        # Send webhook request
        response = requests.post(
            "https://sevvy-web-git-demo-sevvy.vercel.app/api/webhooks/airflow",
            json=webhook_payload,
            headers={"Content-Type": "application/json"},
            timeout=30
        )
        
        if response.status_code == 200:
            print(f"‚úÖ DAG Success webhook sent successfully: {response.status_code}")
            logging.info(f"DAG Success webhook sent successfully: {response.status_code}")
        else:
            print(f"‚ö†Ô∏è DAG Success webhook response: {response.status_code} - {response.text}")
            logging.warning(f"DAG Success webhook non-200 response: {response.status_code} - {response.text}")
            
    except Exception as e:
        print(f"‚ùå Error sending DAG success webhook: {str(e)}")
        logging.error(f"Error sending DAG success webhook: {str(e)}")
        # Don't raise exception - webhook failure shouldn't fail the pipeline


# Default arguments for the DAG
default_args = {
    'owner': 'sevvy-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 7, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

# Create the DAG using TaskFlow API
@dag(
    'etf_holdings_pipeline',
    default_args=default_args,
    description='ETL pipeline for calculating ETF holdings values in USD',
    schedule='@daily',
    catchup=False,
    tags=['etl', 'finance', 'etf', 'production'],
)
def etf_holdings_pipeline():
    """Main DAG definition using TaskFlow API"""
    
    @task()
    def read_etf_market_data(**context) -> List[Dict[str, Any]]:
        """
        Read ETF market data (prices) from etf_market_data table for the execution date
        """
        execution_date = context.get('execution_date') or context.get('logical_date')
        business_date = execution_date.strftime('%Y-%m-%d')
        
        print(f"üìä Reading ETF market data for business date: {business_date}")
        
        # Get database connection
        pg_hook = PostgresHook(postgres_conn_id='pipeline_test_rds')
        
        # Query to fetch ETF prices with deduplication
        query = """
            SELECT DISTINCT ON (price_date, symbol) 
                price_date, symbol, price, currency, asset_id, asset_weight
            FROM public.etf_market_data
            WHERE price_date = %s
            ORDER BY price_date, symbol
        """
        
        # Execute query
        results = pg_hook.get_records(query, parameters=[business_date])
        
        # Convert to list of dictionaries
        market_data = []
        for row in results:
            market_data.append({
                'price_date': str(row[0]),
                'symbol': row[1],
                'price': float(row[2]),
                'currency': row[3],
                'asset_id': row[4],
                'asset_weight': float(row[5]) if row[5] else None
            })
        
        print(f"‚úÖ Found {len(market_data)} ETF prices for {business_date}")
        logging.info(f"Found {len(market_data)} ETF prices for {business_date}")
        
        return market_data
    
    @task()
    def read_exchange_rates(**context) -> Dict[str, float]:
        """
        Read exchange rates from exchange_rates table
        """
        execution_date = context.get('execution_date') or context.get('logical_date')
        business_date = execution_date.strftime('%Y-%m-%d')
        
        print(f"üí± Reading exchange rates for business date: {business_date}")
        
        # Get database connection
        pg_hook = PostgresHook(postgres_conn_id='pipeline_test_rds')
        
        # Query to fetch exchange rates
        query = """
            SELECT from_currency, to_currency, rate
            FROM public.exchange_rates
            WHERE business_date = %s AND to_currency = 'USD'
        """
        
        # Execute query
        results = pg_hook.get_records(query, parameters=[business_date])
        
        # Convert to dictionary for easy lookup
        rates = {}
        for row in results:
            from_currency = row[0]
            rate = float(row[2])
            rates[from_currency] = rate
        
        print(f"‚úÖ Found {len(rates)} exchange rates to USD")
        logging.info(f"Found {len(rates)} exchange rates: {rates}")
        
        return rates
    
    @task()
    def read_etf_trades(**context) -> List[Dict[str, Any]]:
        """
        Read ETF trades (shares) from etf_trades table for the execution date
        """
        execution_date = context.get('execution_date') or context.get('logical_date')
        business_date = execution_date.strftime('%Y-%m-%d')
        
        print(f"üìà Reading ETF trades for business date: {business_date}")
        
        # Get database connection
        pg_hook = PostgresHook(postgres_conn_id='pipeline_test_rds')
        
        # Query to fetch ETF trades
        query = """
            SELECT trade_id, trade_date, etf_symbol, shares
            FROM public.etf_trades
            WHERE trade_date = %s
            ORDER BY etf_symbol
        """
        
        # Execute query
        results = pg_hook.get_records(query, parameters=[business_date])
        
        # Convert to list of dictionaries
        trades = []
        for row in results:
            trades.append({
                'trade_id': row[0],
                'trade_date': str(row[1]),
                'etf_symbol': row[2],
                'shares': float(row[3])
            })
        
        print(f"‚úÖ Found {len(trades)} ETF trades for {business_date}")
        logging.info(f"Found {len(trades)} ETF trades for {business_date}")
        
        return trades
    
    @task()
    def calculate_usd_prices(market_data: List[Dict[str, Any]], 
                            exchange_rates: Dict[str, float]) -> List[Dict[str, Any]]:
        """
        Calculate USD prices for all ETFs
        """
        print(f"üßÆ Calculating USD prices for {len(market_data)} ETFs")
        
        usd_prices = []
        
        for etf in market_data:
            currency = etf['currency']
            original_price = etf['price']
            
            # Calculate USD price
            if currency == 'USD':
                usd_price = original_price
            elif currency in exchange_rates:
                rate = exchange_rates[currency]
                usd_price = original_price * rate
                print(f"  ETF {etf['symbol']}: "
                      f"{original_price:,.2f} {currency} "
                      f"√ó {rate} = {usd_price:,.2f} USD")
            else:
                # Handle missing exchange rate
                logging.warning(f"No exchange rate found for {currency}, "
                               f"setting USD price to 0 for ETF {etf['symbol']}")
                usd_price = 0
            
            usd_prices.append({
                'price_date': etf['price_date'],
                'symbol': etf['symbol'],
                'original_price': original_price,
                'original_currency': currency,
                'usd_price': usd_price,
                'asset_id': etf.get('asset_id'),
                'asset_weight': etf.get('asset_weight')
            })
        
        print(f"‚úÖ Calculated USD prices for {len(usd_prices)} ETFs")
        logging.info(f"Calculated USD prices for {len(usd_prices)} ETFs")
        
        return usd_prices
    
    @task()
    def calculate_holdings(usd_prices: List[Dict[str, Any]], 
                          trades: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Calculate holdings values by matching trades with prices
        """
        print(f"üí∞ Calculating holdings values")
        
        holdings = []
        total_value = 0
        
        for trade in trades:
            trade_symbol = trade['etf_symbol']
            trade_shares = trade['shares']
            business_date = trade['trade_date']
            
            matched = False
            
            for price in usd_prices:
                if price['symbol'] == trade_symbol:
                    holding_value = trade_shares * price['usd_price']
                    
                    holdings.append({
                        'business_date': business_date,
                        'etf_symbol': trade_symbol,
                        'shares': trade_shares,
                        'price_usd': price['usd_price'],
                        'holding_value_usd': holding_value,
                        'trade_id': trade['trade_id'],
                        'price_date': price['price_date'],
                        'asset_id': price.get('asset_id'),
                        'asset_weight': price.get('asset_weight')
                    })
                    
                    total_value += holding_value
                    matched = True
                    
                    print(f"  {trade_symbol}: {trade_shares:,.2f} shares √ó ${price['usd_price']:,.2f} = ${holding_value:,.2f}")
            
            if not matched:
                logging.warning(f"No price found for trade {trade['trade_id']} (symbol: {trade_symbol})")
        
        print(f"‚úÖ Calculated {len(holdings)} holding records")
        print(f"   Total portfolio value: ${total_value:,.2f}")
        logging.info(f"Calculated {len(holdings)} holdings with total value: {total_value}")
        
        return holdings
    
    @task()
    def write_etf_holdings(holdings: List[Dict[str, Any]], **context):
        """
        Write calculated holdings to etf_holdings table
        """
        print(f"üíæ Writing {len(holdings)} holdings to etf_holdings table")
        
        # Initialize variables for webhook
        holdings_written = 0
        total_holdings_value = 0
        execution_date = context.get('execution_date') or context.get('logical_date')
        business_date = execution_date.strftime('%Y-%m-%d')
        
        if not holdings:
            print("‚ö†Ô∏è  No holdings to write")
            # Still send webhook notification even with no data
            send_success_webhook(context, holdings_written, total_holdings_value, business_date)
            return
        
        # Get database connection
        pg_hook = PostgresHook(postgres_conn_id='pipeline_test_rds')
        
        # Aggregate holdings by symbol for final output
        etf_totals = {}
        business_date = holdings[0]['business_date'] if holdings else business_date
        
        for holding in holdings:
            symbol = holding['etf_symbol']
            if symbol not in etf_totals:
                etf_totals[symbol] = 0
            etf_totals[symbol] += holding['holding_value_usd']
        
        # Insert query for simplified schema
        insert_query = """
            INSERT INTO public.etf_holdings 
                (business_date, etf_symbol, amount_usd)
            VALUES (%s, %s, %s)
            ON CONFLICT (business_date, etf_symbol) DO UPDATE SET
                amount_usd = EXCLUDED.amount_usd
        """
        
        # Execute inserts in a transaction
        conn = pg_hook.get_conn()
        cursor = conn.cursor()
        
        try:
            for symbol, amount in etf_totals.items():
                cursor.execute(insert_query, (
                    business_date,
                    symbol,
                    amount
                ))
                
                holdings_written += 1
                total_holdings_value += amount
            
            # Commit the transaction
            conn.commit()
            
            print(f"‚úÖ Successfully wrote {holdings_written} holdings to etf_holdings table")
            logging.info(f"Wrote {holdings_written} holdings with total value: {total_holdings_value}")
            
            # Push statistics to XCom for success callback
            context['ti'].xcom_push(key='holdings_processed', value=holdings_written)
            context['ti'].xcom_push(key='total_holdings_value', value=total_holdings_value)
            context['ti'].xcom_push(key='business_date', value=business_date)
            
            # Send success webhook after successful write
            send_success_webhook(context, holdings_written, total_holdings_value, business_date)
            
        except Exception as e:
            conn.rollback()
            print(f"‚ùå Error writing to etf_holdings: {str(e)}")
            logging.error(f"Database insert failed: {str(e)}")
            raise
        finally:
            cursor.close()
            conn.close()
    
    # Define task dependencies with optimized parallel execution
    market_data = read_etf_market_data()
    rates = read_exchange_rates()
    trades = read_etf_trades()
    
    # Calculate USD prices after market data and rates are ready
    usd_prices = calculate_usd_prices(market_data, rates)
    
    # Calculate holdings after both USD prices and trades are ready
    holdings = calculate_holdings(usd_prices, trades)
    
    # Write holdings to database
    write_etf_holdings(holdings)

# Instantiate the DAG
dag = etf_holdings_pipeline()